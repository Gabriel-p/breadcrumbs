

# Bayesian inference

* [The Model Complexity Myth](https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/)
> An oft-repeated rule of thumb in any sort of statistical model fitting is "you can't fit a model with more parameters than data points". This idea appears to be as wide-spread as it is incorrect. On the contrary, if you construct your models carefully, **you can fit models with more parameters than datapoints**...

The trick are Bayesian priors, and their frequentist equivalent "ridge regularization".